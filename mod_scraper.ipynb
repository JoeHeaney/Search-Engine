{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "4sz5IsD8HmoO",
        "outputId": "336ac522-1c2e-4ac3-bf74-0425b30b8828"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-f1265a42185e>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Increment the page number for the next iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mpage_number\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "base_url = \"https://modrinth.com/mods\"\n",
        "output_file = \"name.txt\"\n",
        "output_file2 = \"authors.txt\"\n",
        "output_file3 = \"descrips.txt\"\n",
        "output_file4 = \"paragraph_descriptions.txt\"\n",
        "items_per_page = 20  # Number of items displayed per page\n",
        "\n",
        "def scrape_names(page_content):\n",
        "    soup = BeautifulSoup(page_content, 'html.parser')\n",
        "    elements = soup.find_all(\"h2\", class_=\"name\")\n",
        "    names = [element.text.strip() for element in elements]\n",
        "    return names\n",
        "\n",
        "def scrape_authors(page_content):\n",
        "    soup = BeautifulSoup(page_content, 'html.parser')\n",
        "    titles = soup.find_all(\"a\",class_=\"title-link\")\n",
        "    authors = [title.text.strip() if title.text.strip() else \"\" for title in titles]\n",
        "    return authors\n",
        "\n",
        "def scrape_descriptions(page_content):\n",
        "    soup = BeautifulSoup(page_content, 'html.parser')\n",
        "    category = soup.find_all(\"div\",class_=\"categories tags\")\n",
        "    descrip = [\", \".join(categories.stripped_strings) for categories in category]\n",
        "    return descrip\n",
        "\n",
        "def scrape_about(page_content):\n",
        "    soup = BeautifulSoup(page_content, 'html.parser')\n",
        "    about = soup.find_all(\"p\", class_=\"description\")\n",
        "    about_desc = [' '.join(paragraph.stripped_strings).replace('\\n', ' ').strip() for paragraph in about]\n",
        "    return about_desc\n",
        "\n",
        "start_page_number = 20 // items_per_page + 1  # Calculate the start page number\n",
        "page_number = start_page_number\n",
        "\n",
        "while True:\n",
        "    url = f\"{base_url}?o={(page_number - 1) * items_per_page}\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Extract and write the names to the file\n",
        "        with open(output_file, \"a\", encoding=\"utf-8\") as file:\n",
        "            names = scrape_names(response.text)\n",
        "            for name in names:\n",
        "                file.write(name + \"\\n\")\n",
        "\n",
        "        with open(output_file2, \"a\", encoding=\"utf-8\") as file:\n",
        "            authors = scrape_authors(response.text)\n",
        "            for author in authors:\n",
        "                file.write(author + \"\\n\")\n",
        "\n",
        "        with open(output_file3, \"a\", encoding=\"utf-8\") as file:\n",
        "            descrip = scrape_descriptions(response.text)\n",
        "            for descrips in descrip:\n",
        "                file.write(descrips + \"\\n\")\n",
        "\n",
        "        with open(output_file4, \"a\", encoding=\"utf-8\") as file:\n",
        "            paragraph_descriptions = scrape_about(response.text)\n",
        "            for paragraph in paragraph_descriptions:\n",
        "                file.write(paragraph + \"\\n\")\n",
        "\n",
        "        time.sleep(5)\n",
        "        page_number += 1\n",
        "    else:\n",
        "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
        "        break\n",
        "\n",
        "print(\"Scraping complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gIDUXHb5Sygm"
      }
    }
  ]
}
